% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{apacite}

% KM added 1/4/18 to allow control of blind submission


\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Frequency-dependent regularization arises from a noisy-channel
processing model}


\author{{\large \bf Zachary Houghton (znhoughton@ucdavis.edu)} \\ Department of Linguistics, 1 Shields Avenue \\ Davis, CA 95616 USA \AND {\large \bf Emily Morgan (eimorgan@ucdavis.edu)} \\ Department of Linguistics, 1 Shields Avenue \\ Davis, CA 95616 USA}

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}%
  {}%
  {\par}

\begin{document}

\maketitle

\begin{abstract}
Language often has different ways to express the same or similar
meanings. Despite this, however, people seem to have preferences for
some ways over others. For example, people overwhelmingly prefer
\emph{bread and butter} to \emph{butter and bread}. Previous research
has demonstrated that these ordering preferences grow stronger with
frequency (i.e., frequency-dependent regularization). In this paper we
demonstrate that this frequency-dependent regularization can be
accounted for by noisy-channel processing models (e.g., Gibson, Bergen,
\& Piantadosi, 2013a; Levy, 2008). We also show that this regularization
can only be accounted for if the listener infers more noise than the
speaker produces. Finally, we show that the model can account for the
language-wide distribution of binomial ordering preferences.

\textbf{Keywords:}
Frequency-dependent regularization; Noisy-channel processing;
Psycholinguistics.
\end{abstract}

\section{Introduction}\label{introduction}

Speakers are often confronted with many different ways to express the
same meaning. A customer might ask whether a store sells ``radios and
televisions'', but they could have just as naturally asked whether the
store sells ``televisions and radios.'' However, despite conveying the
same meaning, speakers sometimes have strong preferences for one choice
over competing choices (e.g., preference for \emph{men and women} over
\emph{women and men}, Benor \& Levy, 2006; Morgan \& Levy, 2016a). These
preferences are driven to some extent by generative preferences (e.g.,
preference for short words before long words), however they are
sometimes violated by idiosyncratic preferences (e.g., \emph{ladies and
gentlemen} preferred despite a general men-before-women generative
preference, Morgan \& Levy, 2016b).

Interestingly, ordering preferences for certain constructions, such as
binomial expressions, are often more extreme for higher frequency items
(e.g., \emph{bread and butter}). That is, higher-frequency items
typically have more polarized preferences (Liu \& Morgan, 2020, 2021;
Morgan \& Levy, 2015, 2016b, 2016a). This phenomenon is called
\emph{frequency-dependent regularization}, and while there is evidence
of it in several different constructions, it is still unclear what
processes this phenomenon is driven by. For example, it could be a
consequence of learning processes or a consequence of sentence
processing more broadly. In the present paper we examine whether a
noisy-channel processing model (Gibson et al., 2013a) combined with
transmission across generations (Reali \& Griffiths, 2009) can account
for frequency-dependent regularization.

\subsection{Frequency-dependent
regularization}\label{frequency-dependent-regularization}

Frequency-dependent regularization has been documented for a variety of
different constructions in English (Liu \& Morgan, 2020, 2021; Morgan \&
Levy, 2015, 2016b). For example, Morgan \& Levy (2015) demonstrated that
more frequent binomial expressions (e.g., \emph{bread and butter}) are
more strongly regularized (i.e., are preferred in one order
overwhelmingly more than the alternative). These ordering preferences
are also not simply a result of abstract ordering preferences (e.g.,
short words before long words, Morgan \& Levy, 2016a).

Additionally, Liu \& Morgan (2020) demonstrated this effect holds true
for the dative alternation in English (e.g., \emph{give} \emph{the ball
to him}~vs \emph{give him the ball}). Specifically, they demonstrated
higher frequency verbs have more polarized preferences with respect to
the dative alternation. Similarly, Liu \& Morgan (2021) showed that
Adjective-Adjective-Noun orderings also show frequency-dependent
regularization. That is, adjective-adjective-Nouns with higher overall
frequencies show stronger ordering preferences, even after taking into
account generative preferences of adjective orderings.

How does this polarization for high-frequency items arise? One
possibility is that it occurs as a consequence of imperfect transmission
between generations. For example, as speakers transmit the language from
one generation to the next, it is possible that the next generation may
infer the probability of each ordering imperfectly. Indeed, Morgan \&
Levy (2016b) demonstrated this possibility in an iterated-learning
paradigm. They showed that frequency-dependent regularization can arise
from an interaction between a frequency-independent bias and
transmission across generations. Specifically, they used an iterated
learning paradigm (following Reali \& Griffiths, 2009) and demonstrated
that by introducing a frequency-independent regularization bias, after
several generations the model predicted frequency-\emph{dependent}
regularization. For example, after hearing \emph{bread and butter} 7
times, and \emph{butter and bread} 3 times, the learner might reproduce
\emph{bread and butter} 8 times and \emph{butter and bread} 2 times.
They argued that frequency-dependent regularization emerges because for
low-frequency items, the regularization bias cannot overcome the prior.
In other words, the model posits that while learners of a language are
in general very good at learning the statistical patterns in the
language (Saffran, Aslin, \& Newport, 1996; Yu \& Smith, 2007), they may
do so imperfectly. Despite the evidence of frequency-dependent
regularization, however, it is unclear what process in language is
analogous to the frequency-independent regularization bias.

\subsection{Noisy-channel Processing}\label{noisy-channel-processing}

One possibility is that frequency-dependent regularization arises as a
product of noisy-channel processing (Gibson et al., 2013a). Listeners
are confronted with a great deal of noise in the form of perception
errors (e.g., a noisy environment) and even production errors (speakers
don't always say what they intended to, Gibson et al., 2013a). In order
to overcome these errors, a processing system must take into account the
noise of the system, for example by probabilistically determining
whether the perceived utterance was infact intended by the speaker.

Indeed, there is evidence that our processing system does take noise
into account. For example, Ganong (1980) found that people will process
a non-word as being a word under noisy conditions. Additionally, Albert
Felty, Buchwald, Gruenenfelder, \& Pisoni (2013) demonstrated that when
listeners do misperceive a word, the word that they believe to have
heard tends to be higher frequency than the target word. Further, Keshev
\& Meltzer-Asscher (2021) found that in Arabic, readers will even
process ambiguous subject/object relative clauses as the more frequent
interpretation, even if this interpretation compromises subject-verb
agreement. These results taken together suggests that misperceptions may
sometimes actually be a consequence of noisy-channel processing (though
see Ferreira \& Patson, 2007 for an alternative account).

Further, people will even process \emph{grammatical} utterances, as a
more frequent or plausible interpretation (Christianson, Hollingworth,
Halliwell, \& Ferreira, 2001; Levy, 2008; Poppels \& Levy, 2016;
\textbf{christian?}), even going so far as to process two
interpretations that cannot both be consistent with the original
sentence. For example, Christianson et al. (2001) demonstrated that when
people read the sentence \emph{While the man hunted the deer ran into
the woods}, people will answer in the affirmative for both \emph{Did the
man hunt the deer?} and \emph{Did the dear run into the woods?}. Levy
(2008) argued that this phenonenon was explained by noisy-channel
processing, since a single insertion results in plausible, grammatical
constructions for both meanings (\emph{While the man hunted it the deer
ran into the woods} vs \emph{While the man hunted the deer it ran into
the woods}).

In order to account for findings like these, Gibson et al. (2013a)
developed a computational model that demonstrated how a system might
take into account noise (see Levy, 2008 for a similar approach).
Specifically, their model operationalizes noisy-channel processing as a
Bayesian process where a listener estimates the probability that their
perception matches the speaker's intended utterance. Specifically, this
is operationlized as being proportional to the prior probability of the
intended utterance multiplied by the probability of the intended
utterance being corrupted to the perceived utterance (See Equation
\ref{eq:gibsonnoisy}):

\begin{equation}
\label{eq:gibsonnoisy}
P(S_i|S_p) \propto P(S_i) P(S_i \to S_p)
\end{equation}

\noindent where \(P(S_i|S_p)\) is the probability that the intended
utterance was actually the utterance that was perceived, \(P(S_i)\) is
the prior probability of the intended utterance, and \(P(S_i \to S_p)\)
is the probability of the perceived utterance (\(S_p\)) given the
intended utterance (\(S_i\)). For example, if \(S_p =\) \emph{bread and
butter}, then in order to infer the probability \emph{bread and butter}
being the intended utterance of the speaker (\(P(S_i)=\) \emph{bread and
butter)}, we multiply the prior probability of \emph{bread and butter,}
\(P(S_i)\), by the probability of the perceived utterance being
\emph{bread and butter} given the intended utterance being \emph{bread
and butter,} \(P(S_p =\) \emph{bread and butter} \textbar{} \(S_i=\)
\emph{bread and butter}). Another way to conceptualize \(P(S_p =\)
\emph{bread and butter} \textbar{} \(S_i=\) \emph{bread and butter}) is
1 - the probability that the speaker said something \emph{other} than
\emph{bread and butter}, given that they intended to say \emph{bread and
butter}.

Gibson et al. (2013a)'s model made a variety of interesting predictions.
For example, the model predicted that when people are presented with an
implausible sentence (e.g., \emph{the mother gave the candle the
daughter}), they should be more likely to interpret the plausible
version of the sentence (e.g., \emph{the mother gave the candle to the
daughter}) if there is increased noise (e.g., by adding syntactic errors
to the filler items, such as a deleted function word). Their model also
predicted that increasing the likelihood of implausible events (e.g., by
adding more filler items that were implausible, such as \emph{the girl
was kicked by the ball}) should increase the rate of implausible
interpretations of the sentence. Interestingly both of these results
were born out in their experimental data. In a follow up study, Poppels
\& Levy (2016) further demonstrated that word-exchanges (e.g., \emph{The
ball kicked the girl} vs \emph{The girl kicked the ball}) are also taken
into account by comprehenders. These results taken together suggest that
humans do utilize a noisy-channel system in processing.

\subsection{Present Study}\label{present-study}

Given the evidence of noisy-channel processing, it is possible that the
frequency-dependent regularization that Morgan \& Levy (2016b) saw is a
product of listeners' noisy-channel processing. That is, perhaps when
learners hear the phrase \emph{butter and bread}, they think the speaker
intended \emph{bread and butter}, and thus also activates \emph{bread
and butter} even though they didn't hear it. Further, this may compound
over time for high frequency items, but not for low frequency items.
Thus, the present study examines whether Gibson et al. (2013a)'s
noisy-channel processing model can also predict frequency-dependent
regularization across generations of language transmission.

\section{Dataset}\label{dataset}

Following Morgan \& Levy (2016b), we use Morgan \& Levy (2015)'s corpus
of 594 binomial expressions. This corpus has been annotated for various
phonological, semantic, and lexical constraints that are known to affect
binomial ordering preferences. The corpus also includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The estimated generative preferences for each binomial, which are
  values between 0 and 1 representing the ordering preferences estimated
  from the above constraints, independent of frequency. The generative
  constraints are calculated using Morgan \& Levy (2015)'s model.
\item
  The observed binomial orderings preferences (hereafter: observed
  preferences) which are the proportion of binomial orderings that are
  in alphabetical order (a neutral reference order) for a given
  binomial. A visualization of the distribution of observed preferences
  and compositional preferences is included below in Figure
  \ref{fig:corpusplot1}.
\item
  The overall frequency of a binomial expression (the frequency of AandB
  plus the frequency of BandA).
\end{enumerate}

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/corpus_plots} 

}

\caption[The left plot is a plot of the observed orderings of binomials in the corpus data from Morgan \& Levy (2015), the right is the plot of the generative preferences of binomials in the same corpus]{The left plot is a plot of the observed orderings of binomials in the corpus data from Morgan \& Levy (2015), the right is the plot of the generative preferences of binomials in the same corpus.}\label{fig:corpusplot1}
\end{figure}
\end{CodeChunk}

\section{Model}\label{model}

Following Morgan \& Levy (2016b), we use a 2-alternative iterated
learning paradigm. At very step, learners hear N tokens with some in
alphabetical (AandB) and some in nonalphabetical (BandA) order. After
hearing all N tokens, the learner then produces N tokens to the next
generation.

Within a single generation, after hearing a single token, learners
compute \(P(S_i = AandB|S_p)\) and update their beliefs about prior
probability of the ordering for a given binomial, \(P(S_i)\). The size
of the update is proportional to how probable the learner believes it is
that each order was intended.

After hearing a token, learners compute \(P(S_i = AandB|S_p)\) according
to Equation \ref{eq:gibsonnoisy}. \(P(S_i \to S_p)\) is a fixed noise
parameter, which we will call \(P_{noise}\). \(P_{noise}\) represents
the probability of the perceived binomial ordering being swapped by the
learner (i.e., AandB being swapped to BandA or vice versa). \(P(S_i)\)
represents the learner's prior belief of the probability of the intended
order of the binomial.

To initialize \(P(S_i)\) before the learner hears any data, we used the
mean and concentration parametrization of the beta distribution. The
mean (\(\mu\)) represents the expectation of the distribution (the mean
value of draws from the distribution). The concentration parameter
(\(\nu\)) describes how dense the distribution is.

Before the learner hears any data, \(\mu\) is equal to the generative
preference for the binomial (taken from Morgan \& Levy, 2016b). \(\nu\)
is a free parameter, set to 10 for all simulations in this
paper.\footnote{Changing \(\nu\) does not qualitatively change the
  pattern of the results for any simulations in the paper, as long as
  it's greater than 2.}

We then use \(P(S_i)\) and \(P(noise)\) to compute \(P(S_i|S_p)\). If
the perceived binomial is alphabetical (AandB), we compute the
unnormalized probability of the alphabetical and nonalphabetical
orderings according to the below equations. Note that the process is
comparable if the perceived binomial is nonalphabetical.

\begin{equation}
\label{eq:praw}
P_{raw}(S_i = AandB|S_p = AandB) \\ = P(S_i = AandB) \cdot (1 -  P(noise))
\end{equation}

\begin{equation}
\label{eqprawtwo}
P_{raw}(S_i = BandA|AandB) = (1 - P(S_i = AandB)) \cdot P(noise)
\end{equation}

On the first trial, \(P(S_i = AandB)\) = \(\mu\).

After calculating the unnormalized (raw) probabilities, they are then
normalized:

\scalebox{0.8}{%
normal:
\begin{equation}
\label{eq:phatalpha}
\hat{P}_{\alpha} = \frac{P_{raw}(S_i = AandB|S_p = AandB)}{P_{raw}(S_i = AandB | S_p = AandB) + P_{raw}(S_i = BandA|S_p = AandB)}
\end{equation}}

\begin{equation}
\label{eq:phatnotalpha}
\hat{P}_{\neg\alpha} = 1 - \hat{P}(\alpha)
\end{equation}

\noindent where \(\hat{P_\alpha}\) is the probability that the intended
binomial order was the alphabetical order, and \(\hat{P}_{\neg\alpha}\)
is the probability that the intended binomial order was the
nonalphabetical order.

We then update \(\alpha_1'\) and \(\alpha_2'\) to be used as the
parameters of \(P(S_i)\) when the learner hears the next token. For
updating we use the pseudocount parametrization, where
\(\alpha_1 = \mu \cdot \nu\) and \(\alpha_2 = 1-\mu \cdot \nu\). This
update is done according to the following equation:

\begin{equation}
\label{eq:alpha1}
\alpha_1' = \alpha_1 + \hat{P}(\alpha)
\end{equation}

\begin{equation}
\label{eq:alpha2}
\alpha_2' = \alpha_2 + \hat{p}(\neg\alpha)
\end{equation}

When the learner hears the next token, they use \(\alpha_1'\) and
\(\alpha_2'\) to compute \(P(S_i)\). Note that when the learner hears a
binomial, they update their beliefs about the probability of both the
alpahabetical \emph{and} nonalphabetical forms of the binomial.

When the learner is done hearing N tokens and updating their beliefs of
\(P(S_i)\) for a given binomial, they then produce N tokens for the next
generation of learners. These are generated binomially, with
\(\theta = P(S_i=AandB)\) is the inferred probability of the
alphabetical form of a given binomial.

When producing each token, there is also a possibility that the speaker
makes an error and produces an unintended ordering of the binomial. In
order to model this, the speaker produces a token in the unintended
order with probability \(P_{SpeakerNoise}\). This is a fixed parameter
in the model and remains constant across binomials and generations.

This process continues iteratively for \(ngen\) generations.

\section{Results}\label{results}

We present our results in two main sections. The first section
demonstrates the effects of the speaker and listener noise parameters
(\(P_{noise}\) and \(P_{SpeakerNoise}\) respectively) on simulations of
individual binomials. The goal of this section is to examine whether the
model can account for frequency-dependent regularization across
individual binomials varying in frequency.

The second section compared our model's predicted binomial orderings
across a range of binomials to the real-world corpus-wide distribution.
In this section, rather than simulating individual binomials, we
simulated the distribution of binomial orderings across the entire
dataset of binomials from Morgan \& Levy (2015) with the intent of
examining whether our model can capture the corpus-wide distribution.

\subsection{Speaker vs Listener Noise}\label{speaker-vs-listener-noise}

First we demonstrate that frequency-dependent regularization does not
arise when there is no listener or speaker noise.\footnote{All code and
  results can be found publicly available here: {[}redacted{]}} Instead
we see convergence to the prior, which is expected. That is, Griffiths
\& Kalish (2007) demonstrated that when learners sample from the
posterior, as the number of iterations increases, the stationary
distribution converges to the prior. In other words, without any noise,
each generation of learners produces data that is more and more similar
to the prior, until convergence is reached. To confirm this, we
simulated the evolution of a single binomial across 500 generations and,
with various frequencies (50, 100, 500, 1000, and 10,000). The
generative preference was 0.6. 1000 chains were run. We then examined
the model's inferred ordering preference in the final generation. A
visualization of the results is presented in Figure
\ref{fig:noNoisePlot}.

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/noNoise} 

}

\caption[A plot of the distribution of simulated binomials at the 500th generation, varying in frequency]{A plot of the distribution of simulated binomials at the 500th generation, varying in frequency. The top value represents N. On the x-axis is the predicted probability of producing the binomial in alphabetical form. On the y-axis is probability density. Speaker and listener noise was set to 0. The generative preference was 0.6, and nu was set to 10. 1000 chains were run. Note that there is no frequency-dependent regularization apparent.}\label{fig:noNoisePlot}
\end{figure}
\end{CodeChunk}

In a follow-up simulation, we introduced speaker and listener noise to
examine the evolution of a single binomial across 500 generations.
Similarly, we varied the frequencies (50, 100, 500, 1000, and 10,000),
set the generative preference to 0.6, and ran 1000 chains. The key
difference is that in this simulation, speaker noise
(\(P_{SpeakerNoise}\)) was set when we to 0.001 and listener noise
\(P_{noise}\) was set to 0.01. Interestingly, introducing noise results
in predicted frequency-dependent regularization across generations
(Figure \ref{fig:regularizationplot1}).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/speaker_noise_001_listener_01} 

}

\caption[A plot of the distribution of simulated binomials at the 500th generation, varying in frequency]{A plot of the distribution of simulated binomials at the 500th generation, varying in frequency. The top value represents N. On the x-axis is the predicted probability of producing the binomial in alphabetical form. On the y-axis is probability density. Speaker noise was set to 0.001, listener noise was set to 0.01, the generative preference was 0.6, and nu was set to 10. 1000 chains were run. Note how for the binomials with large N, the ordering preferences tend to be more extreme.}\label{fig:regularizationplot1}
\end{figure}
\end{CodeChunk}

Further, the disparity of the noise affects the rate of regularization
(see Figure \ref{fig:absolutediff}). To demonstrate this, we replicated
the same simulations as above but with different speaker and listener
values. Specifically, we ran a simulation with the speaker noise
parameter set to 0.091 and the listener noise set to 0.1 (top plot in
\ref{fig:absolutediff}) and another with speaker noise set to 0.001 and
listener noise set to 0.01 (bottom plot in Figure
\ref{fig:absolutediff}). Note that the difference between the speaker
and listener noise for both was the same.

Interestingly, a larger difference between the speaker and listener
noise parameters increases the strength of the regularization. To
examine this, we simulated across the same frequency span as before (50,
100, 500, 1000, 10000), with generative preference set to 0.6, however
we varied the speaker noise parameter. Since regularization occurred
more slowly, we ran the simulation for 2000 generations to ensure that
we arrived at the stationary distribution. For both simulations,
listener noise was set to 0.01, however in one simulation (top of Figure
\ref{fig:fasterslowerreg}), speaker noise was set to 0.009 and in
another simulation (bottom of Figure \ref{fig:fasterslowerreg}), speaker
noise was set to 0.0075. As Figure \ref{fig:fasterslowerreg}
demonstrates, Increased noise results in weaker regularization (i.e.,
less regularization for lower frequency items in the left plot compared
to the right plot).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/fasterSlowerReg} 

}

\caption[A plot of simulations with different noise parameters after 2000 generations]{A plot of simulations with different noise parameters after 2000 generations. For the top plot, the speaker noise was set to 0.009 and the listener noise parameter was set to 0.01. For the bottom plot, the speaker noise was set to 0.0075 and the listener noise parameter was set to 0.01. For both plots, the generative preference was set to 0.6 and nu was set to 10.}\label{fig:fasterslowerreg}
\end{figure}
\end{CodeChunk}

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/absolute_matters} 

}

\caption[A plot of simulations with different noise parameters, but the same difference between the speaker and listener noise parameters]{A plot of simulations with different noise parameters, but the same difference between the speaker and listener noise parameters. For the top plot, the speaker noise was set to 0.091 and listener noise was set to 0.1. For the bottom plot, the speaker noise was set to 0.001 and listener noise was set to 0.01. Note that the difference between the listener and speaker noise parameters for both plots was the same (0.009).}\label{fig:absolutediff}
\end{figure}
\end{CodeChunk}

Interestingly this regularization disappears if the listener's noise
parameter is less than or equal to the speaker's noise parameter. To
examine this, we ran a simulation using the same frequency and
generative preferences settings as before, but we varied the listener
and speaker noise parameters. Specifically, listener noise was set to
0.001 and speaker noise was set to 0.01. We ran the simulation across
500 generations. The simulations demonstrate that when listener noise is
less than speaker noise, regularization does not occur (Figure
\ref{fig:regularizationplot2}).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/speaker_noise_01_listener_001} 

}

\caption[A plot of the distribution of simulated binomials at the 500th generation, varying in frequency]{A plot of the distribution of simulated binomials at the 500th generation, varying in frequency. The top value represents N. On the x-axis is the predicted probability of producing the binomial in alphabetical form. On the y-axis is probability density. Speaker noise was set to 0.01, listener noise was set to 0.001, the generative preference was 0.6, and nu was set to 10. 1000 chains were run. Note how regularization does not appear to be present in this graph.}\label{fig:regularizationplot2}
\end{figure}
\end{CodeChunk}

It is useful to revisit here what the speaker and listener noise
parameters represent. The speaker noise parameter is how often the
speaker produces an error and the listener noise parameter is the
listeners' belief of how noisy the environment is. Framed this way, it
is perhaps unsurprising that we do not see regularization when the
parameters equal eachother, since they essentially cancel eachother out
(everytime a speaker makes an error, the listener is accounting for it,
thus we get convergence to the prior).

Thus our model makes a novel prediction: In order to account for
frequency-dependent regularization, listeners must be inferring more
noise than speakers are actually producing (according to our model).

\subsection{Corpus Data}\label{corpus-data}

Finally, we now demonstrate that our model also predicts the
language-wide distribution of binomial preference strengths seen in the
corpus data. In order to demonstrate this, we simulated model
predictions for the 594 binomials from Morgan \& Levy (2015). The model
estimated the ordering preference across 500 generations with 10 chains
each. The generative preference and N was dependent on the binomial. We
also set listener noise to 0.02 and speaker noise to 0.005. Our results
demonstrate that our model can approximate the distribution in the
corpus data (See Figure \ref{fig:corpusourmodel}).

\begin{CodeChunk}
\begin{figure}[tb]

{\centering \includegraphics[width=1\linewidth]{Figures/corpus_plot_and_ours} 

}

\caption[A plot of the distribution of ordering preferences after 500 generations of our iterated learning model (left) and the stationary distribution of ordering preferences in the corpus data from Morgan \& Levy (2015)]{A plot of the distribution of ordering preferences after 500 generations of our iterated learning model (left) and the stationary distribution of ordering preferences in the corpus data from Morgan \& Levy (2015). For our simulations, the binomial frequencies and generative preferences were matched with the corpus data. $
u$ was set to 10, listener noise was set to 0.02, and speaker noise was set to 0.005.}\label{fig:corpusourmodel}
\end{figure}
\end{CodeChunk}

\section{Conclusion}\label{conclusion}

The present study examined whether a noisy-channel processing model
(Gibson, Bergen, \& Piantadosi, 2013b) integrated in an iterated
learning model (Morgan \& Levy, 2016b) can capture the effects of
frequency-dependent regularization.

Our results demonstrate the frequency-dependent regularization can
emerge from a noisy-channel processing model when listeners encounter
more noise in the environment than the speakers actually produce.

Further our results suggest that in order to account for
frequency-dependent regularization, listeners are encountering more
noise than speakers are producing. An interesting avenue for future
research is whether this prediction is born out in experimental work.

\section{References}\label{references}

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}

\noindent

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-feltyMisperceptionsSpokenWords}
Albert Felty, R., Buchwald, A., Gruenenfelder, T. M., \& Pisoni, D. B.
(2013). Misperceptions of spoken words: Data from a random sample of
american english words. \emph{The Journal of the Acoustical Society of
America}, \emph{134}(1), 572--585.

\bibitem[\citeproctext]{ref-benor2006}
Benor, S. B., \& Levy, R. (2006). The chicken or the egg? A
probabilistic analysis of english binomials. \emph{Language}, 233278.

\bibitem[\citeproctext]{ref-christiansonThematicRolesAssigned2001}
Christianson, K., Hollingworth, A., Halliwell, J. F., \& Ferreira, F.
(2001). Thematic roles assigned along the garden path linger.
\emph{Cognitive Psychology}, \emph{42}(4), 368--407.
http://doi.org/\href{https://doi.org/10.1006/cogp.2001.0752}{10.1006/cogp.2001.0752}

\bibitem[\citeproctext]{ref-ferreiraGoodEnoughApproach2007}
Ferreira, F., \& Patson, N. D. (2007). The {`}Good Enough{'} Approach to
Language Comprehension. \emph{Language and Linguistics Compass},
\emph{1}(1-2), 71--83.
http://doi.org/\href{https://doi.org/10.1111/j.1749-818X.2007.00007.x}{10.1111/j.1749-818X.2007.00007.x}

\bibitem[\citeproctext]{ref-ganongPhoneticCategorizationAuditory1980}
Ganong, W. F. (1980). Phonetic categorization in auditory word
perception. \emph{Journal of Experimental Psychology: Human Perception
and Performance}, \emph{6}(1), 110. Retrieved from
\url{https://psycnet.apa.org/record/1981-07020-001}

\bibitem[\citeproctext]{ref-gibson2013}
Gibson, E., Bergen, L., \& Piantadosi, S. T. (2013b). Rational
integration of noisy evidence and prior semantic expectations in
sentence interpretation. \emph{Proceedings of the National Academy of
Sciences}, \emph{110}(20), 8051--8056.
http://doi.org/\href{https://doi.org/10.1073/pnas.1216438110}{10.1073/pnas.1216438110}

\bibitem[\citeproctext]{ref-gibsonNoisy2013}
Gibson, E., Bergen, L., \& Piantadosi, S. T. (2013a). Rational
integration of noisy evidence and prior semantic expectations in
sentence interpretation. \emph{Proceedings of the National Academy of
Sciences}, \emph{110}(20), 8051--8056.
http://doi.org/\href{https://doi.org/10.1073/pnas.1216438110}{10.1073/pnas.1216438110}

\bibitem[\citeproctext]{ref-griffithsLanguageEvolutionIterated2007}
Griffiths, T. L., \& Kalish, M. L. (2007). Language evolution by
iterated learning with bayesian agents. \emph{Cognitive Science},
\emph{31}(3), 441--480.
http://doi.org/\href{https://doi.org/10.1080/15326900701326576}{10.1080/15326900701326576}

\bibitem[\citeproctext]{ref-keshevNoisyBetterRare2021}
Keshev, M., \& Meltzer-Asscher, A. (2021). Noisy is better than rare:
Comprehenders compromise subject-verb agreement to form more probable
linguistic structures. \emph{Cognitive Psychology}, \emph{124}, 101359.
http://doi.org/\href{https://doi.org/10.1016/j.cogpsych.2020.101359}{10.1016/j.cogpsych.2020.101359}

\bibitem[\citeproctext]{ref-levyNoisychannel2008}
Levy, R. (2008). A noisy-channel model of human sentence comprehension
under uncertain input. In (p. 234243). Retrieved from
\url{https://aclanthology.org/D08-1025.pdf}

\bibitem[\citeproctext]{ref-liu2020}
Liu, Z., \& Morgan, E. (2020). Frequency-dependent regularization in
constituent ordering preferences. In. Retrieved from
\url{https://www.cognitivesciencesociety.org/cogsci20/papers/0751/0751.pdf}

\bibitem[\citeproctext]{ref-liu2021}
Liu, Z., \& Morgan, E. (2021). Frequency-dependent regularization in
syntactic constructions. In (p. 387389). Retrieved from
\url{https://aclanthology.org/2021.scil-1.41.pdf}

\bibitem[\citeproctext]{ref-morganModelingIdiosyncraticPreferences2015}
Morgan, E., \& Levy, R. (2015). Modeling idiosyncratic preferences : How
generative knowledge and expression frequency jointly determine language
structure, 1649--1654.

\bibitem[\citeproctext]{ref-morgan2016}
Morgan, E., \& Levy, R. (2016a). Abstract knowledge versus direct
experience in processing of binomial expressions. \emph{Cognition},
\emph{157}, 384--402.
http://doi.org/\href{https://doi.org/10.1016/j.cognition.2016.09.011}{10.1016/j.cognition.2016.09.011}

\bibitem[\citeproctext]{ref-morganFrequencydependentRegularizationIterated2016}
Morgan, E., \& Levy, R. (2016b). Frequency-dependent regularization in
iterated learning. \emph{The Evolution of Language: Proceedings of the
11th International Conference}.

\bibitem[\citeproctext]{ref-poppelsStructuresensitiveNoiseInference2016}
Poppels, T., \& Levy, R. (2016). Structure-sensitive noise inference:
Comprehenders expect exchange errors. In. Retrieved from
\url{https://tpoppels.github.io/files/2016-poppels-levy-cogsci-proceedings.pdf}

\bibitem[\citeproctext]{ref-realiEvolutionFrequencyDistributions2009}
Reali, F., \& Griffiths, T. L. (2009). The evolution of frequency
distributions: Relating regularization to inductive biases through
iterated learning. \emph{Cognition}, \emph{111}(3), 317--328.
http://doi.org/\href{https://doi.org/10.1016/j.cognition.2009.02.012}{10.1016/j.cognition.2009.02.012}

\bibitem[\citeproctext]{ref-saffranStatisticalLearning8MonthOld1996}
Saffran, J. R., Aslin, R. N., \& Newport, E. L. (1996). Statistical
Learning by 8-Month-Old Infants. \emph{Science}, \emph{274}(5294),
1926--1928.
http://doi.org/\href{https://doi.org/10.1126/science.274.5294.1926}{10.1126/science.274.5294.1926}

\bibitem[\citeproctext]{ref-yuRapidWordLearning2007}
Yu, C., \& Smith, L. B. (2007). Rapid Word Learning Under Uncertainty
via Cross-Situational Statistics. \emph{Psychological Science},
\emph{18}(5), 414--420.
http://doi.org/\href{https://doi.org/10.1111/j.1467-9280.2007.01915.x}{10.1111/j.1467-9280.2007.01915.x}

\end{CSLReferences}

\bibliographystyle{apacite}


\end{document}
